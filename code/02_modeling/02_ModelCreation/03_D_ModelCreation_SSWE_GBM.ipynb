{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The purpose of this notebook is to train a **Gradient Boosting** based model to classify the tweets' sentiment as positive or negative. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random_seed=1\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "import keras\n",
    "from sklearn import  metrics\n",
    "from timeit import default_timer as timer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import os, io\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Embedding, Merge, Dropout \n",
    "from keras import backend as K\n",
    "import unicodedata \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.externals import joblib\n",
    "import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the training file'\n",
    "data_dir = r'C:\\Users\\ds1\\Documents\\AzureML\\data'\n",
    "\n",
    "# Path of the word vectors\n",
    "vectors_file = r'../02_modeling/vectors/embeddings_SSWE_Basic_Keras_w_CNTK.tsv' \n",
    "\n",
    "model_identifier='evaluation_SSWE_gbm'\n",
    "models_dir='model'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "vector_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the data\n",
    "\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Þ\",\":-Þ\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"ʘ‿ʘ\",\"❤\",\"💜\",\"💚\",\"💕\",\"💙\",\"💛\",\"💓\",\"💝\",\"💖\",\"💞\",\n",
    "               \"💘\",\"💗\",\"😗\",\"😘\",\"😙\",\"😚\",\"😻\",\"😀\",\"😁\",\"😃\",\"☺\",\"😄\",\"😆\",\"😇\",\"😉\",\"😊\",\"😋\",\"😍\",\n",
    "               \"😎\",\"😏\",\"😛\",\"😜\",\"😝\",\"😮\",\"😸\",\"😹\",\"😺\",\"😻\",\"😼\",\"👍\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(¬_¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"💔\",\"☹️\",\"😌\",\"😒\",\"😓\",\"😔\",\"😕\",\"😖\",\"😞\",\"😟\",\n",
    "               \"😠\",\"😡\",\"😢\",\"😣\",\"😤\",\"😥\",\"😦\",\"😧\",\"😨\",\"😩\",\"😪\",\"😫\",\"😬\",\"😭\",\"😯\",\"😰\",\"😱\",\"😲\",\n",
    "               \"😳\",\"😴\",\"😷\",\"😾\",\"😿\",\"🙀\",\"💀\",\"👎\"]\n",
    "\n",
    "# Emails\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "# Mentions\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "\n",
    "#Urls\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "#Numerics\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
    "\n",
    "emoticonsDict = {} # define desired replacements here\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens\"\"\"   \n",
    "    with open(filename, 'r') as f:\n",
    "    \n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "                    line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "                    line = userMentionsRegex.sub(' USER ', line )\n",
    "                    line = emailsRegex.sub(' EMAIL ', line )\n",
    "                    line=urlsRegex.sub(' URL ', line)\n",
    "                    line=numsRegex.sub(' NUM ',line)\n",
    "                    line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
    "                    line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "                    words_tokens=[token for token in TweetTokenizer().tokenize(line)]                   \n",
    "                    line= ' '.join(token for token in words_tokens )        \n",
    "                    padded_lines.append(line)\n",
    "        return padded_lines\n",
    "    \n",
    "def read_labels(filename):\n",
    "    \"\"\" read the tweet labels from the file\"\"\"\n",
    "    arr= np.genfromtxt(filename, delimiter='\\n')\n",
    "    arr[arr==4]=1 # Encode the positive category as 1\n",
    "    return arr\n",
    "\n",
    "# Convert Word Embedding to Sentence Embedding\n",
    "\n",
    "def load_word_embedding(vectors_file):\n",
    "    \"\"\" Load the word vectors\"\"\"\n",
    "    vectors= np.genfromtxt(vectors_file, delimiter='\\t', comments='#--#',dtype=None,\n",
    "                           names=['Word']+['EV{}'.format(i) for i in range(1,51)])\n",
    "    # comments have to be changed as some of the tokens are having # in them and then we dont need comments\n",
    "    vectors_dc={}\n",
    "    for x in vectors:\n",
    "        vectors_dc[x['Word'].decode('utf-8','ignore')]=[float(x[each]) for each in ['EV{}'.format(i) for i in range(1,51)]]\n",
    "    return vectors_dc\n",
    "\n",
    "def get_sentence_embedding(text_data, vectors_dc):\n",
    "    sentence_vectors=[]\n",
    "    \n",
    "    for sen in text_data:\n",
    "        tokens=sen.split(' ')\n",
    "        current_vector=np.array([vectors_dc[tokens[0]] if tokens[0] in vectors_dc else vectors_dc['<UNK>']])\n",
    "        for word in tokens[1:]:\n",
    "            if word in vectors_dc:\n",
    "                current_vector=np.vstack([current_vector,vectors_dc[word]])\n",
    "            else:\n",
    "                current_vector=np.vstack([current_vector,vectors_dc['<UNK>']])\n",
    "        min_max_mean=np.hstack([current_vector.min(axis=0),current_vector.max(axis=0),current_vector.mean(axis=0)])\n",
    "        sentence_vectors.append(min_max_mean)\n",
    "    return sentence_vectors\n",
    "\n",
    "# Model Training\n",
    "\n",
    "def heldout_score(clf, X_test, y_test,n_estimators =20):\n",
    "    \"\"\"compute deviance scores on ``X_test`` and ``y_test``. \"\"\"\n",
    "    score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        score[i] = clf.loss_(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "def cv_estimate(n_splits,X_train, y_train,n_estimators =20):\n",
    "    best_score, best_model= None,None\n",
    "    cv = KFold(n_splits=n_splits)\n",
    "    cv_clf = ensemble.GradientBoostingClassifier(n_estimators=n_estimators,min_samples_leaf=3, verbose=1, loss='deviance')\n",
    "    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    i=0\n",
    "    for train, test in cv.split(X_train, y_train):\n",
    "        cv_clf.fit(X_train[train], y_train[train])\n",
    "        current_score= heldout_score(cv_clf, X_train[test], y_train[test],n_estimators)\n",
    "        val_scores += current_score\n",
    "        print ('Fold {} Score {}'.format(i+1, np.mean(current_score)))\n",
    "        if i==0:\n",
    "            best_score=np.mean(current_score)\n",
    "            best_model=cv_clf\n",
    "        else:\n",
    "            \n",
    "            if np.mean(current_score)<best_score:\n",
    "                best_score=np.mean(current_score)\n",
    "                best_model=cv_clf\n",
    "        i+=1\n",
    "    val_scores /= n_splits\n",
    "    return val_scores, best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: Loading Training data\n",
      "Step 2 : Load word vectors\n",
      "Step 3: Convert Word vectors to sentence vectors\n",
      "1280000 1280000 1280000\n",
      " Encoding the input\n",
      "Step 4: Gradient Boosting Model using sklearn\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3342           12.57m\n",
      "         2           1.2912           12.11m\n",
      "         3           1.2557           11.41m\n",
      "         4           1.2260           10.75m\n",
      "         5           1.2008            9.80m\n",
      "         6           1.1796            9.07m\n",
      "         7           1.1617            8.29m\n",
      "         8           1.1465            7.57m\n",
      "         9           1.1334            6.93m\n",
      "        10           1.1222            6.28m\n",
      "        20           1.0607            0.00s\n",
      "Fold 1 Score 1.1455005499864506\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3345           11.19m\n",
      "         2           1.2916           10.41m\n",
      "         3           1.2561           10.06m\n",
      "         4           1.2264            9.58m\n",
      "         5           1.2014            9.10m\n",
      "         6           1.1803            8.39m\n",
      "         7           1.1624            7.84m\n",
      "         8           1.1471            7.25m\n",
      "         9           1.1341            6.60m\n",
      "        10           1.1228            5.98m\n",
      "        20           1.0619            0.00s\n",
      "Fold 2 Score 1.1444809418074342\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3344           10.46m\n",
      "         2           1.2916           10.18m\n",
      "         3           1.2561            9.74m\n",
      "         4           1.2262            9.28m\n",
      "         5           1.2012            8.71m\n",
      "         6           1.1802            8.14m\n",
      "         7           1.1623            7.56m\n",
      "         8           1.1471            6.99m\n",
      "         9           1.1340            6.44m\n",
      "        10           1.1226            5.83m\n",
      "        20           1.0609            0.00s\n",
      "Fold 3 Score 1.1444253069369759\n",
      "Step 5: Save the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model\\\\evaluation_SSWE_gbm']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Step1: Loading Training data')\n",
    "train_texts=read_data(data_dir+'\\\\training_text.csv')\n",
    "train_labels=read_labels(data_dir+'\\\\training_label.csv')\n",
    "\n",
    "print ('Step 2 : Load word vectors')\n",
    "vectors_dc=load_word_embedding(vectors_file)\n",
    "len(vectors_dc)\n",
    "\n",
    "print ('Step 3: Convert Word vectors to sentence vectors')\n",
    "train_sentence_vectors=get_sentence_embedding(train_texts,vectors_dc)\n",
    "print (len(train_sentence_vectors), len(train_labels), len(train_texts))\n",
    "\n",
    "print (\" Encoding the input\")\n",
    "train_x=train_sentence_vectors\n",
    "train_y=train_labels\n",
    "train_x=np.array(train_x).astype('float32')\n",
    "train_y=np.array(train_y)\n",
    "\n",
    "print ('Step 4: Gradient Boosting Model using sklearn')\n",
    "n_splits=3\n",
    "cv_score,best_model = cv_estimate(n_splits,train_x, train_y,20)\n",
    "\n",
    "print ('Step 5: Save the model')\n",
    "joblib.dump(best_model, models_dir+'\\\\'+model_identifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

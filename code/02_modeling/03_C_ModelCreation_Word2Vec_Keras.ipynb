{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The purpose of this notebook is to train a **Logistic Regression** model using Keras to classify the tweets' sentiment as positive or negative.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "random_seed=1\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Embedding , Activation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import num2words\n",
    "from timeit import default_timer as timer\n",
    "from sklearn import  metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the training file'\n",
    "data_dir = r'C:\\Users\\ds1\\Documents\\AzureML\\data'  \n",
    "\n",
    "# Path of the word vectors\n",
    "vectors_file = r'../02_modeling/vectors/embeddings_Word2Vec_Basic.tsv'\n",
    "model_identifier='evaluation_word2vec_logistic'\n",
    "models_dir='model'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Data Preprocessing\n",
    "\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Ã\",\":-Ã\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"Ê˜â€¿Ê˜\",\"â¤\",\"ğŸ’œ\",\"ğŸ’š\",\"ğŸ’•\",\"ğŸ’™\",\"ğŸ’›\",\"ğŸ’“\",\"ğŸ’\",\"ğŸ’–\",\"ğŸ’\",\n",
    "               \"ğŸ’˜\",\"ğŸ’—\",\"ğŸ˜—\",\"ğŸ˜˜\",\"ğŸ˜™\",\"ğŸ˜š\",\"ğŸ˜»\",\"ğŸ˜€\",\"ğŸ˜\",\"ğŸ˜ƒ\",\"â˜º\",\"ğŸ˜„\",\"ğŸ˜†\",\"ğŸ˜‡\",\"ğŸ˜‰\",\"ğŸ˜Š\",\"ğŸ˜‹\",\"ğŸ˜\",\n",
    "               \"ğŸ˜\",\"ğŸ˜\",\"ğŸ˜›\",\"ğŸ˜œ\",\"ğŸ˜\",\"ğŸ˜®\",\"ğŸ˜¸\",\"ğŸ˜¹\",\"ğŸ˜º\",\"ğŸ˜»\",\"ğŸ˜¼\",\"ğŸ‘\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(Â¬_Â¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_Â´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"ğŸ’”\",\"â˜¹ï¸\",\"ğŸ˜Œ\",\"ğŸ˜’\",\"ğŸ˜“\",\"ğŸ˜”\",\"ğŸ˜•\",\"ğŸ˜–\",\"ğŸ˜\",\"ğŸ˜Ÿ\",\n",
    "               \"ğŸ˜ \",\"ğŸ˜¡\",\"ğŸ˜¢\",\"ğŸ˜£\",\"ğŸ˜¤\",\"ğŸ˜¥\",\"ğŸ˜¦\",\"ğŸ˜§\",\"ğŸ˜¨\",\"ğŸ˜©\",\"ğŸ˜ª\",\"ğŸ˜«\",\"ğŸ˜¬\",\"ğŸ˜­\",\"ğŸ˜¯\",\"ğŸ˜°\",\"ğŸ˜±\",\"ğŸ˜²\",\n",
    "               \"ğŸ˜³\",\"ğŸ˜´\",\"ğŸ˜·\",\"ğŸ˜¾\",\"ğŸ˜¿\",\"ğŸ™€\",\"ğŸ’€\",\"ğŸ‘\"]\n",
    "\n",
    "# Emails\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "# Mentions\n",
    "\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "#Urls\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "#Numerics\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "\n",
    "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
    "\n",
    "\n",
    "emoticonsDict = {} # define desired replacements here\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Read the raw tweet data from a file. Replace Emails etc with special tokens\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "    \n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "                    line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "                    line = userMentionsRegex.sub(' USER ', line )\n",
    "                    line = emailsRegex.sub(' EMAIL ', line )\n",
    "                    line=urlsRegex.sub(' URL ', line)\n",
    "                    line=numsRegex.sub(' NUM ',line)\n",
    "                    line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
    "                    line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "                    words_tokens=[token for token in TweetTokenizer().tokenize(line)]                   \n",
    "                    line= ' '.join(token for token in words_tokens )        \n",
    "                    padded_lines.append(line)\n",
    "        return padded_lines\n",
    "    \n",
    "def read_labels(filename):\n",
    "    \"\"\" read the tweet labels from the file\n",
    "    \"\"\"\n",
    "    arr= np.genfromtxt(filename, delimiter='\\n')\n",
    "    arr[arr==4]=1 # Encode the positive category as 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "# # Convert Word Vectors to Sentence Vectors\n",
    "\n",
    "# The embeddings generated by both SSWE and Word2Vec algorithms are at word level but as we are using the sentences as the input, the word embeddings need to be converted to the sentence level embeddings. We are converting the word embeddings into sentence embeddings by using the approach in the original SSWE paper i.e. stacking the word vectors into a matrix and applying min, max and average operations on each of the columns of the word vectors matrix.\n",
    "\n",
    "def load_word_embedding(vectors_file):\n",
    "    \"\"\" Load the word vectors\"\"\"\n",
    "    vectors= np.genfromtxt(vectors_file, delimiter='\\t', comments='#--#',dtype=None,\n",
    "                           names=['Word']+['EV{}'.format(i) for i in range(1,51)])\n",
    "    # comments have to be changed as some of the tokens are having # in them and then we dont need comments\n",
    "    vectors_dc={}\n",
    "    for x in vectors:\n",
    "        vectors_dc[x['Word'].decode('utf-8','ignore')]=[float(x[each]) for each in ['EV{}'.format(i) for i in range(1,51)]]\n",
    "    return vectors_dc\n",
    "\n",
    "def get_sentence_embedding(text_data, vectors_dc):\n",
    "    \"\"\" This function converts the vectors of all the words in a sentence into sentence level vectors\"\"\"\n",
    "    \"\"\" This function stacks up all the words vectors and then applies min, max and average operations over the stacked vectors\"\"\"\n",
    "    \"\"\" If the size of the words vectors is n, then the size of the sentence vectors would be 3*n\"\"\"\n",
    "    sentence_vectors=[]\n",
    "    \n",
    "    for sen in text_data:\n",
    "        tokens=sen.split(' ')\n",
    "        current_vector=np.array([vectors_dc[tokens[0]] if tokens[0] in vectors_dc else vectors_dc['<UNK>']])\n",
    "        for word in tokens[1:]:\n",
    "            if word in vectors_dc:\n",
    "                current_vector=np.vstack([current_vector,vectors_dc[word]])\n",
    "            else:\n",
    "                current_vector=np.vstack([current_vector,vectors_dc['<UNK>']])\n",
    "        min_max_mean=np.hstack([current_vector.min(axis=0),current_vector.max(axis=0),current_vector.mean(axis=0)])\n",
    "        sentence_vectors.append(min_max_mean)\n",
    "    return sentence_vectors\n",
    "\n",
    "# # Model Training\n",
    "\n",
    "\n",
    "batch_size = 1028*6 # Batch Size should be changed according to the system specifications to have better utilization of GPU\n",
    "nb_epoch = 30\n",
    "    \n",
    "def init_model():\n",
    "    output_dim = no_classes = len(to_categorical(train_y)[0])\n",
    "    input_dim=150\n",
    "    model = Sequential() \n",
    "    model.add(Dense(output_dim, input_dim=input_dim, activation='softmax',activity_regularizer=regularizers.l1_l2(1))) \n",
    "    return model\n",
    "\n",
    "def cv_estimate(n_splits,X_train, y_train):\n",
    "    best_score, best_model= None,None\n",
    "    cv = KFold(n_splits=n_splits)\n",
    "    \n",
    "    i=0\n",
    "    for train, test in cv.split(X_train, y_train):\n",
    "        model=init_model()\n",
    "        mcp = ModelCheckpoint('./model_chkpoint_{}'.format(i), monitor=\"val_acc\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "        model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "        model.fit(X_train[train], to_categorical(y_train[train]),epochs=nb_epoch,batch_size=batch_size, callbacks=[mcp],\n",
    "                  validation_split=0.2)\n",
    "        current_score= model.evaluate(X_train[test], to_categorical(y_train[test]))[0] # Getting the loss\n",
    "        print ('\\n Fold {} Current score {}'.format(i+1, current_score))\n",
    "        \n",
    "        \n",
    "        if i==0:\n",
    "            best_score=current_score\n",
    "            best_model=model\n",
    "        else:\n",
    "\n",
    "            if current_score<best_score:\n",
    "                best_score=current_score\n",
    "                best_model=model\n",
    "        i+=1\n",
    "\n",
    "    return  best_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Training data\n",
      "Step 2: Load Word Vectors\n",
      "Step 3: Converting the word vectors to sentence vectors\n",
      " Encoding the data\n",
      "1280000 1280000 1280000\n",
      "Step 4: Logistic regression model using Keras\n",
      "Train on 682666 samples, validate on 170667 samples\n",
      "Epoch 1/30\n",
      "682666/682666 [==============================] - 2s - loss: 6187.3993 - acc: 0.4682 - val_loss: 6150.0293 - val_acc: 0.4707\n",
      "Epoch 2/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3624 - acc: 0.4753 - val_loss: 6150.0194 - val_acc: 0.4781\n",
      "Epoch 3/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3540 - acc: 0.4813 - val_loss: 6150.0124 - val_acc: 0.4829\n",
      "Epoch 4/30\n",
      "682666/682666 [==============================] - 2s - loss: 6187.3478 - acc: 0.4859 - val_loss: 6150.0072 - val_acc: 0.4874\n",
      "Epoch 5/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3433 - acc: 0.4900 - val_loss: 6150.0033 - val_acc: 0.4923\n",
      "Epoch 6/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3398 - acc: 0.4939 - val_loss: 6150.0001 - val_acc: 0.4967\n",
      "Epoch 7/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3370 - acc: 0.4977 - val_loss: 6149.9976 - val_acc: 0.4994\n",
      "Epoch 8/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3347 - acc: 0.5011 - val_loss: 6149.9955 - val_acc: 0.5021\n",
      "Epoch 9/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3328 - acc: 0.5042 - val_loss: 6149.9938 - val_acc: 0.5053\n",
      "Epoch 10/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3313 - acc: 0.5071 - val_loss: 6149.9924 - val_acc: 0.5082\n",
      "Epoch 11/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3299 - acc: 0.5104 - val_loss: 6149.9912 - val_acc: 0.5113\n",
      "Epoch 12/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3288 - acc: 0.5131 - val_loss: 6149.9901 - val_acc: 0.5151\n",
      "Epoch 13/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3278 - acc: 0.5157 - val_loss: 6149.9892 - val_acc: 0.5172\n",
      "Epoch 14/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3270 - acc: 0.5184 - val_loss: 6149.9885 - val_acc: 0.5194\n",
      "Epoch 15/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3262 - acc: 0.5209 - val_loss: 6149.9877 - val_acc: 0.5223\n",
      "Epoch 16/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3256 - acc: 0.5235 - val_loss: 6149.9872 - val_acc: 0.5247\n",
      "Epoch 17/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3250 - acc: 0.5263 - val_loss: 6149.9865 - val_acc: 0.5280\n",
      "Epoch 18/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3245 - acc: 0.5291 - val_loss: 6149.9861 - val_acc: 0.5306\n",
      "Epoch 19/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3240 - acc: 0.5314 - val_loss: 6149.9857 - val_acc: 0.5326\n",
      "Epoch 20/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3237 - acc: 0.5339 - val_loss: 6149.9854 - val_acc: 0.5351\n",
      "Epoch 21/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3233 - acc: 0.5361 - val_loss: 6149.9850 - val_acc: 0.5365\n",
      "Epoch 22/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3230 - acc: 0.5384 - val_loss: 6149.9848 - val_acc: 0.5395\n",
      "Epoch 23/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3226 - acc: 0.5407 - val_loss: 6149.9844 - val_acc: 0.5415\n",
      "Epoch 24/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3225 - acc: 0.5429 - val_loss: 6149.9842 - val_acc: 0.5437\n",
      "Epoch 25/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3222 - acc: 0.5449 - val_loss: 6149.9838 - val_acc: 0.5457\n",
      "Epoch 26/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3220 - acc: 0.5467 - val_loss: 6149.9838 - val_acc: 0.5473\n",
      "Epoch 27/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3218 - acc: 0.5489 - val_loss: 6149.9836 - val_acc: 0.5498\n",
      "Epoch 28/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3215 - acc: 0.5509 - val_loss: 6149.9833 - val_acc: 0.5512\n",
      "Epoch 29/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3214 - acc: 0.5523 - val_loss: 6149.9833 - val_acc: 0.5533\n",
      "Epoch 30/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3214 - acc: 0.5543 - val_loss: 6149.9831 - val_acc: 0.5552\n",
      "424320/426667 [============================>.] - ETA: 0s\n",
      " Fold 1 Current score 32.85119678948185\n",
      "Train on 682666 samples, validate on 170667 samples\n",
      "Epoch 1/30\n",
      "682666/682666 [==============================] - 2s - loss: 6187.3729 - acc: 0.5059 - val_loss: 6150.0239 - val_acc: 0.4989\n",
      "Epoch 2/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3587 - acc: 0.5001 - val_loss: 6150.0170 - val_acc: 0.4977\n",
      "Epoch 3/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3524 - acc: 0.4999 - val_loss: 6150.0114 - val_acc: 0.4980\n",
      "Epoch 4/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3475 - acc: 0.5007 - val_loss: 6150.0071 - val_acc: 0.4992\n",
      "Epoch 5/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3435 - acc: 0.5023 - val_loss: 6150.0035 - val_acc: 0.5009\n",
      "Epoch 6/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3402 - acc: 0.5040 - val_loss: 6150.0006 - val_acc: 0.5026\n",
      "Epoch 7/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3375 - acc: 0.5059 - val_loss: 6149.9981 - val_acc: 0.5044\n",
      "Epoch 8/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3351 - acc: 0.5078 - val_loss: 6149.9959 - val_acc: 0.5065\n",
      "Epoch 9/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3333 - acc: 0.5097 - val_loss: 6149.9942 - val_acc: 0.5084\n",
      "Epoch 10/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3316 - acc: 0.5117 - val_loss: 6149.9928 - val_acc: 0.5112\n",
      "Epoch 11/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3302 - acc: 0.5141 - val_loss: 6149.9914 - val_acc: 0.5134\n",
      "Epoch 12/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3290 - acc: 0.5164 - val_loss: 6149.9903 - val_acc: 0.5158\n",
      "Epoch 13/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3280 - acc: 0.5185 - val_loss: 6149.9893 - val_acc: 0.5186\n",
      "Epoch 14/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3271 - acc: 0.5204 - val_loss: 6149.9886 - val_acc: 0.5208\n",
      "Epoch 15/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3262 - acc: 0.5228 - val_loss: 6149.9877 - val_acc: 0.5237\n",
      "Epoch 16/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3256 - acc: 0.5255 - val_loss: 6149.9872 - val_acc: 0.5255\n",
      "Epoch 17/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3250 - acc: 0.5280 - val_loss: 6149.9866 - val_acc: 0.5280\n",
      "Epoch 18/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3244 - acc: 0.5302 - val_loss: 6149.9861 - val_acc: 0.5302\n",
      "Epoch 19/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3239 - acc: 0.5327 - val_loss: 6149.9856 - val_acc: 0.5333\n",
      "Epoch 20/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3235 - acc: 0.5350 - val_loss: 6149.9851 - val_acc: 0.5362\n",
      "Epoch 21/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3231 - acc: 0.5375 - val_loss: 6149.9848 - val_acc: 0.5382\n",
      "Epoch 22/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3227 - acc: 0.5399 - val_loss: 6149.9845 - val_acc: 0.5401\n",
      "Epoch 23/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3224 - acc: 0.5425 - val_loss: 6149.9841 - val_acc: 0.5422\n",
      "Epoch 24/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3221 - acc: 0.5445 - val_loss: 6149.9840 - val_acc: 0.5445\n",
      "Epoch 25/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3220 - acc: 0.5470 - val_loss: 6149.9835 - val_acc: 0.5478\n",
      "Epoch 26/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3216 - acc: 0.5493 - val_loss: 6149.9834 - val_acc: 0.5498\n",
      "Epoch 27/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3215 - acc: 0.5510 - val_loss: 6149.9834 - val_acc: 0.5527\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682666/682666 [==============================] - 1s - loss: 6187.3213 - acc: 0.5534 - val_loss: 6149.9828 - val_acc: 0.5543\n",
      "Epoch 29/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3211 - acc: 0.5550 - val_loss: 6149.9829 - val_acc: 0.5569\n",
      "Epoch 30/30\n",
      "682666/682666 [==============================] - 1s - loss: 6187.3209 - acc: 0.5568 - val_loss: 6149.9828 - val_acc: 0.5582\n",
      "423968/426667 [============================>.] - ETA: 0s\n",
      " Fold 2 Current score 32.85117363706287\n",
      "Train on 682667 samples, validate on 170667 samples\n",
      "Epoch 1/30\n",
      "682667/682667 [==============================] - 2s - loss: 6187.3717 - acc: 0.4913 - val_loss: 6150.0227 - val_acc: 0.4948\n",
      "Epoch 2/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3600 - acc: 0.4962 - val_loss: 6150.0142 - val_acc: 0.4994\n",
      "Epoch 3/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3526 - acc: 0.5004 - val_loss: 6150.0080 - val_acc: 0.5039\n",
      "Epoch 4/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3473 - acc: 0.5043 - val_loss: 6150.0034 - val_acc: 0.5068\n",
      "Epoch 5/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3431 - acc: 0.5078 - val_loss: 6149.9999 - val_acc: 0.5102\n",
      "Epoch 6/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3399 - acc: 0.5110 - val_loss: 6149.9970 - val_acc: 0.5138\n",
      "Epoch 7/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3373 - acc: 0.5140 - val_loss: 6149.9947 - val_acc: 0.5177\n",
      "Epoch 8/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3353 - acc: 0.5176 - val_loss: 6149.9930 - val_acc: 0.5207\n",
      "Epoch 9/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3336 - acc: 0.5208 - val_loss: 6149.9914 - val_acc: 0.5246\n",
      "Epoch 10/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3322 - acc: 0.5242 - val_loss: 6149.9901 - val_acc: 0.5276\n",
      "Epoch 11/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3310 - acc: 0.5270 - val_loss: 6149.9890 - val_acc: 0.5306\n",
      "Epoch 12/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3300 - acc: 0.5304 - val_loss: 6149.9882 - val_acc: 0.5337\n",
      "Epoch 13/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3291 - acc: 0.5337 - val_loss: 6149.9872 - val_acc: 0.5369\n",
      "Epoch 14/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3284 - acc: 0.5368 - val_loss: 6149.9866 - val_acc: 0.5399\n",
      "Epoch 15/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3277 - acc: 0.5398 - val_loss: 6149.9860 - val_acc: 0.5430\n",
      "Epoch 16/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3271 - acc: 0.5426 - val_loss: 6149.9855 - val_acc: 0.5453\n",
      "Epoch 17/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3267 - acc: 0.5451 - val_loss: 6149.9850 - val_acc: 0.5483\n",
      "Epoch 18/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3262 - acc: 0.5476 - val_loss: 6149.9846 - val_acc: 0.5507\n",
      "Epoch 19/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3258 - acc: 0.5502 - val_loss: 6149.9842 - val_acc: 0.5534\n",
      "Epoch 20/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3255 - acc: 0.5529 - val_loss: 6149.9839 - val_acc: 0.5560\n",
      "Epoch 21/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3251 - acc: 0.5557 - val_loss: 6149.9835 - val_acc: 0.5581\n",
      "Epoch 22/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3249 - acc: 0.5584 - val_loss: 6149.9834 - val_acc: 0.5616\n",
      "Epoch 23/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3245 - acc: 0.5609 - val_loss: 6149.9831 - val_acc: 0.5640\n",
      "Epoch 24/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3244 - acc: 0.5636 - val_loss: 6149.9830 - val_acc: 0.5662\n",
      "Epoch 25/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3242 - acc: 0.5658 - val_loss: 6149.9827 - val_acc: 0.5688\n",
      "Epoch 26/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3239 - acc: 0.5679 - val_loss: 6149.9824 - val_acc: 0.5704\n",
      "Epoch 27/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3239 - acc: 0.5699 - val_loss: 6149.9825 - val_acc: 0.5721\n",
      "Epoch 28/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3238 - acc: 0.5722 - val_loss: 6149.9823 - val_acc: 0.5748\n",
      "Epoch 29/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3236 - acc: 0.5740 - val_loss: 6149.9820 - val_acc: 0.5767\n",
      "Epoch 30/30\n",
      "682667/682667 [==============================] - 1s - loss: 6187.3234 - acc: 0.5761 - val_loss: 6149.9819 - val_acc: 0.5786\n",
      "423808/426666 [============================>.] - ETA: 0s\n",
      " Fold 3 Current score 32.850874645104604\n",
      "Step 5: Saving the model\n"
     ]
    }
   ],
   "source": [
    "# # Main\n",
    "\n",
    "print ('Step 1: Loading Training data')\n",
    "train_texts=read_data(data_dir+'/training_text.csv')\n",
    "train_labels=read_labels(data_dir+'/training_label.csv')\n",
    "\n",
    "print (\"Step 2: Load Word Vectors\")\n",
    "vectors_dc=load_word_embedding(vectors_file)\n",
    "len(vectors_dc)\n",
    "\n",
    "print (\"Step 3: Converting the word vectors to sentence vectors\")\n",
    "train_sentence_vectors=get_sentence_embedding(train_texts,vectors_dc)\n",
    "\n",
    "print (\" Encoding the data\")\n",
    "train_x=train_sentence_vectors\n",
    "train_y=train_labels\n",
    "train_x=np.array(train_x).astype('float32')\n",
    "train_y=np.array(train_y)\n",
    "print (len(train_sentence_vectors), len(train_labels), len(train_texts))\n",
    "\n",
    "print ('Step 4: Logistic regression model using Keras')\n",
    "best_model=cv_estimate(3,train_x, train_y)\n",
    "\n",
    "print (\"Step 5: Saving the model\")\n",
    "best_model.save(models_dir+'//'+model_identifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

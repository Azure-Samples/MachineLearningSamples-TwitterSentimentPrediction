{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is to train a **Logistic Regression** model using Keras to classify the tweets' sentiment as positive or negative.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "random_seed=1\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Embedding , Activation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import num2words\n",
    "from timeit import default_timer as timer\n",
    "from sklearn import  metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path of the training file'\n",
    "data_dir = r'C:\\Users\\ds1\\Documents\\AzureML\\data'\n",
    "\n",
    "# Path of the word vectors\n",
    "vectors_file = r'../02_modeling/vectors/embeddings_SSWE_Basic_Keras_w_CNTK.tsv' \n",
    "\n",
    "model_identifier='evaluation_SSWE_logistic'\n",
    "models_dir='model'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Þ\",\":-Þ\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"ʘ‿ʘ\",\"❤\",\"💜\",\"💚\",\"💕\",\"💙\",\"💛\",\"💓\",\"💝\",\"💖\",\"💞\",\n",
    "               \"💘\",\"💗\",\"😗\",\"😘\",\"😙\",\"😚\",\"😻\",\"😀\",\"😁\",\"😃\",\"☺\",\"😄\",\"😆\",\"😇\",\"😉\",\"😊\",\"😋\",\"😍\",\n",
    "               \"😎\",\"😏\",\"😛\",\"😜\",\"😝\",\"😮\",\"😸\",\"😹\",\"😺\",\"😻\",\"😼\",\"👍\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(¬_¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"💔\",\"☹️\",\"😌\",\"😒\",\"😓\",\"😔\",\"😕\",\"😖\",\"😞\",\"😟\",\n",
    "               \"😠\",\"😡\",\"😢\",\"😣\",\"😤\",\"😥\",\"😦\",\"😧\",\"😨\",\"😩\",\"😪\",\"😫\",\"😬\",\"😭\",\"😯\",\"😰\",\"😱\",\"😲\",\n",
    "               \"😳\",\"😴\",\"😷\",\"😾\",\"😿\",\"🙀\",\"💀\",\"👎\"]\n",
    "\n",
    "# Emails\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "# Mentions\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "\n",
    "#Urls\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "#Numerics\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
    "\n",
    "emoticonsDict = {}\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "    \n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "                    line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "                    line = userMentionsRegex.sub(' USER ', line )\n",
    "                    line = emailsRegex.sub(' EMAIL ', line )\n",
    "                    line=urlsRegex.sub(' URL ', line)\n",
    "                    line=numsRegex.sub(' NUM ',line)\n",
    "                    line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
    "                    line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "                    words_tokens=[token for token in TweetTokenizer().tokenize(line)]                   \n",
    "                    line= ' '.join(token for token in words_tokens )        \n",
    "                    padded_lines.append(line)\n",
    "        return padded_lines\n",
    "    \n",
    "def read_labels(filename):\n",
    "    \"\"\" read the tweet labels from the file\"\"\"\n",
    "    arr= np.genfromtxt(filename, delimiter='\\n')\n",
    "    arr[arr==4]=1 # Encode the positive category as 1\n",
    "    return arr\n",
    "\n",
    "# Convert Word Vectors to Sentence Vectors\n",
    "\n",
    "def load_word_embedding(vectors_file):\n",
    "    \"\"\" Load the word vectors\"\"\"\n",
    "    vectors= np.genfromtxt(vectors_file, delimiter='\\t', comments='#--#',dtype=None,\n",
    "                           names=['Word']+['EV{}'.format(i) for i in range(1,51)])\n",
    "    # comments have to be changed as some of the tokens are having # in them and then we dont need comments\n",
    "    vectors_dc={}\n",
    "    for x in vectors:\n",
    "        vectors_dc[x['Word'].decode('utf-8','ignore')]=[float(x[each]) for each in ['EV{}'.format(i) for i in range(1,51)]]\n",
    "    return vectors_dc\n",
    "\n",
    "def get_sentence_embedding(text_data, vectors_dc):\n",
    "    sentence_vectors=[]\n",
    "    \n",
    "    for sen in text_data:\n",
    "        tokens=sen.split(' ')\n",
    "        current_vector=np.array([vectors_dc[tokens[0]] if tokens[0] in vectors_dc else vectors_dc['<UNK>']])\n",
    "        for word in tokens[1:]:\n",
    "            if word in vectors_dc:\n",
    "                current_vector=np.vstack([current_vector,vectors_dc[word]])\n",
    "            else:\n",
    "                current_vector=np.vstack([current_vector,vectors_dc['<UNK>']])\n",
    "        min_max_mean=np.hstack([current_vector.min(axis=0),current_vector.max(axis=0),current_vector.mean(axis=0)])\n",
    "        sentence_vectors.append(min_max_mean)\n",
    "    return sentence_vectors\n",
    "\n",
    "\n",
    "# Model Training\n",
    "\n",
    "batch_size = 1028*6 # Batch Size should be changed according to the system specifications to have better utilization of GPU\n",
    "nb_epoch = 10\n",
    "\n",
    "def init_model():\n",
    "    output_dim = no_classes = len(to_categorical(train_y)[0])\n",
    "    input_dim=150\n",
    "    model = Sequential() \n",
    "    model.add(Dense(output_dim, input_dim=input_dim, activation='softmax',activity_regularizer=regularizers.l1_l2(1))) \n",
    "    return model\n",
    "\n",
    "def cv_estimate(n_splits,X_train, y_train):\n",
    "    best_score, best_model= None,None\n",
    "    cv = KFold(n_splits=n_splits)\n",
    "    \n",
    "    i=0\n",
    "    for train, test in cv.split(X_train, y_train):\n",
    "        model=init_model()\n",
    "        mcp = ModelCheckpoint('./model_chkpoint_{}'.format(i), monitor=\"val_acc\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "        model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "        model.fit(X_train[train], to_categorical(y_train[train]),epochs=nb_epoch,batch_size=batch_size, callbacks=[mcp],\n",
    "                  validation_split=0.2)\n",
    "        current_score= model.evaluate(X_train[test], to_categorical(y_train[test]))[0] # Getting the loss\n",
    "        print ('\\n Fold {} Current score {}'.format(i+1, current_score))\n",
    "                \n",
    "        if i==0:\n",
    "            best_score=current_score\n",
    "            best_model=model\n",
    "        else:\n",
    "            print (current_score)\n",
    "            if current_score<best_score:\n",
    "                best_score=current_score\n",
    "                best_model=model\n",
    "        i+=1\n",
    "\n",
    "    return  best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Step 1: Loading Training data')\n",
    "train_texts=read_data(data_dir+'/training_text.csv')\n",
    "print ('Reading training labels')\n",
    "train_labels=read_labels(data_dir+'/training_label.csv')\n",
    "\n",
    "print (\"Step 2: Load Word Vectors\")\n",
    "vectors_dc=load_word_embedding(vectors_file)\n",
    "len(vectors_dc)\n",
    "\n",
    "print (\"Step 3: Converting the word vectors to sentence vectors\")\n",
    "train_sentence_vectors=get_sentence_embedding(train_texts,vectors_dc)\n",
    "print (len(train_sentence_vectors), len(train_labels), len(train_texts))\n",
    "print (\" Encoding the input\")\n",
    "train_x=train_sentence_vectors\n",
    "train_y=train_labels\n",
    "train_x=np.array(train_x).astype('float32')\n",
    "train_y=np.array(train_y)\n",
    "\n",
    "print ('Step 4: Logistic regression model using Keras')\n",
    "best_model=cv_estimate(3,train_x, train_y)\n",
    "\n",
    "print (\"Step 5: Saving the model\")\n",
    "best_model.save(models_dir+'//'+model_identifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

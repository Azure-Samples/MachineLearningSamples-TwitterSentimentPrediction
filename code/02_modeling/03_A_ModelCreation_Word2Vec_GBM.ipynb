{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is to train a gradient boosting based model to classify the tweets' sentiment as positive or negative.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "random_seed=123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers.core import Lambda\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Embedding , Activation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import num2words\n",
    "from timeit import default_timer as timer\n",
    "from sklearn import  metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the training file'\n",
    "data_dir = r'C:\\Users\\ds1\\Documents\\AzureML\\data'  \n",
    "\n",
    "# Path of the word vectors\n",
    "vectors_file = r'../02_modeling/vectors/embeddings_Word2Vec_Basic.tsv'\n",
    "model_identifier = 'evaluation_word2vec_gbm'\n",
    "models_dir = 'model'\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "vector_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the data\n",
    "\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Þ\",\":-Þ\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"ʘ‿ʘ\",\"❤\",\"💜\",\"💚\",\"💕\",\"💙\",\"💛\",\"💓\",\"💝\",\"💖\",\"💞\",\n",
    "               \"💘\",\"💗\",\"😗\",\"😘\",\"😙\",\"😚\",\"😻\",\"😀\",\"😁\",\"😃\",\"☺\",\"😄\",\"😆\",\"😇\",\"😉\",\"😊\",\"😋\",\"😍\",\n",
    "               \"😎\",\"😏\",\"😛\",\"😜\",\"😝\",\"😮\",\"😸\",\"😹\",\"😺\",\"😻\",\"😼\",\"👍\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(¬_¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"💔\",\"☹️\",\"😌\",\"😒\",\"😓\",\"😔\",\"😕\",\"😖\",\"😞\",\"😟\",\n",
    "               \"😠\",\"😡\",\"😢\",\"😣\",\"😤\",\"😥\",\"😦\",\"😧\",\"😨\",\"😩\",\"😪\",\"😫\",\"😬\",\"😭\",\"😯\",\"😰\",\"😱\",\"😲\",\n",
    "               \"😳\",\"😴\",\"😷\",\"😾\",\"😿\",\"🙀\",\"💀\",\"👎\"]\n",
    "\n",
    "# Emails\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "# Mentions\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "\n",
    "#Urls\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "#Numerics\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
    "\n",
    "emoticonsDict = {} # define desired replacements here\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "    \n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "                    line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "                    line = userMentionsRegex.sub(' USER ', line )\n",
    "                    line = emailsRegex.sub(' EMAIL ', line )\n",
    "                    line=urlsRegex.sub(' URL ', line)\n",
    "                    line=numsRegex.sub(' NUM ',line)\n",
    "                    line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
    "                    line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "                    words_tokens=[token for token in TweetTokenizer().tokenize(line)]                   \n",
    "                    line= ' '.join(token for token in words_tokens )        \n",
    "                    padded_lines.append(line)\n",
    "        return padded_lines\n",
    "    \n",
    "def read_labels(filename):\n",
    "    \"\"\" read the tweet labels from the file\"\"\"\n",
    "    arr= np.genfromtxt(filename, delimiter='\\n')\n",
    "    arr[arr==4]=1 # Encode the positive category as 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "# Convert Word Vectors to Sentence Vectors\n",
    "\n",
    "def load_word_embedding(vectors_file):\n",
    "    \"\"\" Load the word vectors\"\"\"\n",
    "    vectors= np.genfromtxt(vectors_file, delimiter='\\t', comments='#--#',dtype=None,\n",
    "                           names=['Word']+['EV{}'.format(i) for i in range(1,51)])\n",
    "    # comments have to be changed as some of the tokens are having # in them and then we dont need comments\n",
    "    vectors_dc={}\n",
    "    for x in vectors:\n",
    "        vectors_dc[x['Word'].decode('utf-8','ignore')]=[float(x[each]) for each in ['EV{}'.format(i) for i in range(1,51)]]\n",
    "    return vectors_dc\n",
    "\n",
    "def get_sentence_embedding(text_data, vectors_dc):\n",
    "    sentence_vectors=[]\n",
    "    \n",
    "    for sen in text_data:\n",
    "        tokens=sen.split(' ')\n",
    "        current_vector=np.array([vectors_dc[tokens[0]] if tokens[0] in vectors_dc else vectors_dc['<UNK>']])\n",
    "        for word in tokens[1:]:\n",
    "            if word in vectors_dc:\n",
    "                current_vector=np.vstack([current_vector,vectors_dc[word]])\n",
    "            else:\n",
    "                current_vector=np.vstack([current_vector,vectors_dc['<UNK>']])\n",
    "        min_max_mean=np.hstack([current_vector.min(axis=0),current_vector.max(axis=0),current_vector.mean(axis=0)])\n",
    "        sentence_vectors.append(min_max_mean)\n",
    "\n",
    "    return sentence_vectors\n",
    "\n",
    "\n",
    "# Model Training\n",
    "\n",
    "def heldout_score(clf, X_test, y_test,n_estimators =20):\n",
    "    \"\"\"compute deviance scores on ``X_test`` and ``y_test``. \"\"\"\n",
    "    score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        score[i] = clf.loss_(y_test, y_pred)\n",
    "    return score\n",
    "\n",
    "def cv_estimate(n_splits,X_train, y_train,n_estimators =20):\n",
    "    best_score, best_model= None,None\n",
    "    cv = KFold(n_splits=n_splits)\n",
    "    cv_clf = GradientBoostingClassifier(n_estimators=n_estimators,min_samples_leaf=3, verbose=1, loss='deviance')\n",
    "    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    i=0\n",
    "    for train, test in cv.split(X_train, y_train):\n",
    "        cv_clf.fit(X_train[train], y_train[train])\n",
    "        current_score= heldout_score(cv_clf, X_train[test], y_train[test],n_estimators)\n",
    "        val_scores += current_score\n",
    "        print ('Fold {} Score {}'.format(i+1, np.mean(current_score)))\n",
    "        if i==0:\n",
    "            best_score=np.mean(current_score)\n",
    "            best_model=cv_clf\n",
    "        else:\n",
    "            if np.mean(current_score)<best_score:\n",
    "                best_score=np.mean(current_score)\n",
    "                best_model=cv_clf\n",
    "        i+=1\n",
    "    val_scores /= n_splits\n",
    "    return val_scores, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: Loading Training data\n"
     ]
    }
   ],
   "source": [
    "print ('Step1: Loading Training data')\n",
    "train_texts=read_data(data_dir+'/training_text.csv')\n",
    "train_labels=read_labels(data_dir+'/training_label.csv')\n",
    "\n",
    "print ('Step 2 : Load word vectors')\n",
    "vectors_dc=load_word_embedding(vectors_file)\n",
    "len(vectors_dc)\n",
    "\n",
    "print ('Step 3: Convert Word vectors to sentence vectors')\n",
    "train_sentence_vectors=get_sentence_embedding(train_texts,vectors_dc)\n",
    "print (len(train_sentence_vectors), len(train_labels), len(train_texts))\n",
    "\n",
    "print (\"Encoding data\")\n",
    "train_x, valid_x, train_y, valid_y=train_test_split(train_sentence_vectors, train_labels, test_size=0.2, random_state=random_seed)\n",
    "train_x=np.array(train_x).astype('float32')\n",
    "valid_x=np.array(valid_x).astype('float32')\n",
    "train_y=np.array(train_y)\n",
    "valid_y=np.array(valid_y)\n",
    "\n",
    "print ('Step 4: Gradient Boosting Modele using sklearn')\n",
    "n_splits=3\n",
    "cv_score,best_model = cv_estimate(n_splits,train_x, train_y,20)\n",
    "\n",
    "print ('Step 5: Save the model')\n",
    "model_identifier = 'evaluation_word2vec_gbm'\n",
    "joblib.dump(best_model, models_dir+'//'+model_identifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

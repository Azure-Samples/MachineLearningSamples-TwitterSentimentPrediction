{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        try:\n",
    "            from importlib import reload\n",
    "            reload(K)  # Python 2.7\n",
    "        except NameError:\n",
    "            try:\n",
    "                from importlib import reload  # Python 3.4+\n",
    "                reload(K)\n",
    "            except ImportError:\n",
    "                from imp import reload  # Python 3.0 - 3.3\n",
    "                reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"cntk\")\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling1D,MaxPooling1D\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.core import Lambda\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import re\n",
    "import io\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import num2words\n",
    "\n",
    "random_seed=1\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\ds1\\Documents\\AzureML\\data'\n",
    "embedding_folder = 'vectors'\n",
    "model_identifier = 'SSWE_Basic_Keras_w_CNTK'\n",
    "\n",
    "if not os.path.exists(embedding_folder):\n",
    "    os.makedirs(embedding_folder)\n",
    "\n",
    "max_sequence_length = 15 # each sentence of the input should be padded to have at least this many tokens\n",
    "embedding_dim \t\t= 50 # Embedding layer size\n",
    "no_filters\t\t\t= 15 # No of filters for the convolution layer\n",
    "filter_size\t\t\t= 5  # Filter size for the convolution layer\n",
    "trainable \t\t\t= True # flag specifying whether the embedding layer weights should be changed during the training or not\n",
    "batch_size \t\t\t= 1024*6 # batch size can be increased to have better gpu utilization\n",
    "#batch_size \t\t\t= 64 # batch size can be increased to have better gpu utilization\n",
    "no_epochs \t\t\t= 5 # No of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Ã\",\":-Ã\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"Ê˜â€¿Ê˜\",\"â¤\",\"ğŸ’œ\",\"ğŸ’š\",\"ğŸ’•\",\"ğŸ’™\",\"ğŸ’›\",\"ğŸ’“\",\"ğŸ’\",\"ğŸ’–\",\"ğŸ’\",\n",
    "               \"ğŸ’˜\",\"ğŸ’—\",\"ğŸ˜—\",\"ğŸ˜˜\",\"ğŸ˜™\",\"ğŸ˜š\",\"ğŸ˜»\",\"ğŸ˜€\",\"ğŸ˜\",\"ğŸ˜ƒ\",\"â˜º\",\"ğŸ˜„\",\"ğŸ˜†\",\"ğŸ˜‡\",\"ğŸ˜‰\",\"ğŸ˜Š\",\"ğŸ˜‹\",\"ğŸ˜\",\n",
    "               \"ğŸ˜\",\"ğŸ˜\",\"ğŸ˜›\",\"ğŸ˜œ\",\"ğŸ˜\",\"ğŸ˜®\",\"ğŸ˜¸\",\"ğŸ˜¹\",\"ğŸ˜º\",\"ğŸ˜»\",\"ğŸ˜¼\",\"ğŸ‘\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(Â¬_Â¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_Â´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"ğŸ’”\",\"â˜¹ï¸\",\"ğŸ˜Œ\",\"ğŸ˜’\",\"ğŸ˜“\",\"ğŸ˜”\",\"ğŸ˜•\",\"ğŸ˜–\",\"ğŸ˜\",\"ğŸ˜Ÿ\",\n",
    "               \"ğŸ˜ \",\"ğŸ˜¡\",\"ğŸ˜¢\",\"ğŸ˜£\",\"ğŸ˜¤\",\"ğŸ˜¥\",\"ğŸ˜¦\",\"ğŸ˜§\",\"ğŸ˜¨\",\"ğŸ˜©\",\"ğŸ˜ª\",\"ğŸ˜«\",\"ğŸ˜¬\",\"ğŸ˜­\",\"ğŸ˜¯\",\"ğŸ˜°\",\"ğŸ˜±\",\"ğŸ˜²\",\n",
    "               \"ğŸ˜³\",\"ğŸ˜´\",\"ğŸ˜·\",\"ğŸ˜¾\",\"ğŸ˜¿\",\"ğŸ™€\",\"ğŸ’€\",\"ğŸ‘\"]\n",
    "\n",
    "# Emails\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "# Mentions\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "\n",
    "#Urls\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "#Numerics\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
    "\n",
    "emoticonsDict = {}\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Labels\n",
      "Loading Training data\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):   \n",
    "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "    \n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "                    line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "                    line = userMentionsRegex.sub(' USER ', line )\n",
    "                    line = emailsRegex.sub(' EMAIL ', line )\n",
    "                    line=urlsRegex.sub(' URL ', line)\n",
    "                    line=numsRegex.sub(' NUM ',line)\n",
    "                    line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
    "                    line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "                    words_tokens=[token for token in TweetTokenizer().tokenize(line)]                   \n",
    "                    line= ' '.join(token for token in words_tokens )        \n",
    "                    padded_lines.append(line)\n",
    "        return padded_lines\n",
    "    \n",
    "    \n",
    "def read_labels(filename):\n",
    "    \"\"\" read the tweet labels from the file\"\"\"\n",
    "    arr= np.genfromtxt(filename, delimiter='\\n')\n",
    "    arr[arr==4]=1 # Encode the positive category as 1\n",
    "    return arr\n",
    "\n",
    "# Loading Training and Validation Data\n",
    "texts \t\t\t\t= []\n",
    "labels \t\t\t\t= []\n",
    "nb_train_samples\t= 0\n",
    "nb_valid_samples \t= 0\n",
    "\n",
    "print ('Loading Training Labels')\n",
    "train_labels=read_labels(data_dir+'\\\\training_label.csv')\n",
    "\n",
    "print ('Loading Training data')\n",
    "train_texts=read_data(data_dir+'//training_text.csv')\n",
    "\n",
    "print (len(train_labels), len(train_texts))\n",
    "print (\"Using Keras tokenizer to tokenize and build word index\")\n",
    "tokenizer = Tokenizer(lower=False, filters='\\n\\t?\"!') \n",
    "train_texts=[each for each in train_texts]\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "sorted_voc = [wc[0] for wc in sorted(tokenizer.word_counts.items(),reverse=True, key= lambda x:x[1]) ]\n",
    "tokenizer.word_index = dict(list(zip(sorted_voc, list(range(2, len(sorted_voc) + 2)))))\n",
    "tokenizer.word_index['<PAD>']=0\n",
    "tokenizer.word_index['<UNK>']=1\n",
    "word_index = tokenizer.word_index\n",
    "reverse_dictionary={v:k for (k,v) in tokenizer.word_index.items()}\n",
    "vocab_size=len(tokenizer.word_index.keys())\n",
    "\n",
    "print ('Size of the vocab is', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling /Padding the data\n",
    "\n",
    "print ('Padding sentences and shuffling the data')\n",
    "sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "\n",
    "#Pad the sentences to have consistent length\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "labels = to_categorical(np.asarray(train_labels))\n",
    "indices = np.arange(len(labels))\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "train_x, valid_x, train_y, valid_y=train_test_split(data, labels, test_size=0.2, random_state=random_seed)\n",
    "train_x=np.array(train_x).astype('float32')\n",
    "valid_x=np.array(valid_x).astype('float32')\n",
    "train_y=np.array(train_y)\n",
    "valid_y=np.array(valid_y)\n",
    "embedding_matrix = np.zeros((len(word_index) , embedding_dim))\n",
    "training_word_index=tokenizer.word_index.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Instantiation\n",
    "print ('Initializing the model')\n",
    "mcp = ModelCheckpoint('./model_chkpoint', monitor=\"val_acc\", save_best_only=True, save_weights_only=False)\n",
    "\n",
    "#Creating network\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+2,\n",
    "                            embedding_dim,\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable, name='embedding'))\n",
    "model.add(Convolution1D(no_filters, filter_size, activation='relu'))\n",
    "model.add(MaxPooling1D(max_sequence_length - filter_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(no_filters, activation='tanh'))\n",
    "model.add(Dense(len(labels[0]), activation='softmax'))\n",
    "\n",
    "optim=optimizers.Adam(lr=0.1, )\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "start=timer()\n",
    "hist=model.fit(train_x, train_y,nb_epoch=no_epochs, batch_size=batch_size,validation_data=(valid_x, valid_y),callbacks=[mcp])\n",
    "end=timer()\n",
    "\n",
    "# Exporting the Embedding Matrix and Vocabulary\n",
    "def export_embeddings(model_orig):\n",
    "    \"\"\" export embeddings to file\"\"\"\n",
    "    embedding_weights=pd.DataFrame(model_orig.layers[0].get_weights()[0]).reset_index()\n",
    "    word_indices_df=pd.DataFrame.from_dict(training_word_index,orient='index').reset_index()\n",
    "    word_indices_df.columns=['word','index']\n",
    "    print (word_indices_df.shape,embedding_weights.shape)\n",
    "    merged=pd.merge(word_indices_df,embedding_weights)\n",
    "    print (merged.shape)\n",
    "    merged=merged[[each for each in merged.columns if each!='index']]    \n",
    "    merged.to_csv(embedding_folder+'//embeddings_{}.tsv'.format(model_identifier), sep='\\t', \n",
    "              index=False, header=False,float_format='%.6f',encoding='utf-8')\n",
    "    return embedding_weights, word_indices_df, merged\n",
    "\n",
    "embedding_weights, word_indices_df, merged_df=export_embeddings(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  \n",
    "import tensorflow as tf\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_filename = r'C:\\Users\\ds1\\Documents\\AzureML\\data\\training_text.csv'\n",
    "model_identifier = 'Word2Vec_Basic'\n",
    "embedding_folder = 'vectors'\n",
    "\n",
    "if not os.path.exists(embedding_folder):\n",
    "    os.makedirs(embedding_folder)\n",
    "\n",
    "batch_size = 128       #64may cause error, re-run or try 128\n",
    "embedding_size = 50   # Dimension of the embedding vector.\n",
    "skip_window = 2       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing and batch preparation\n",
    "# In the following code, we replace Emails, URLS, emoticons etc with special labels\n",
    "\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Þ\",\":-Þ\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"ʘ‿ʘ\",\"❤\",\"💜\",\"💚\",\"💕\",\"💙\",\"💛\",\"💓\",\"💝\",\"💖\",\"💞\",\n",
    "               \"💘\",\"💗\",\"😗\",\"😘\",\"😙\",\"😚\",\"😻\",\"😀\",\"😁\",\"😃\",\"☺\",\"😄\",\"😆\",\"😇\",\"😉\",\"😊\",\"😋\",\"😍\",\n",
    "               \"😎\",\"😏\",\"😛\",\"😜\",\"😝\",\"😮\",\"😸\",\"😹\",\"😺\",\"😻\",\"😼\",\"👍\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(¬_¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"💔\",\"☹️\",\"😌\",\"😒\",\"😓\",\"😔\",\"😕\",\"😖\",\"😞\",\"😟\",\n",
    "               \"😠\",\"😡\",\"😢\",\"😣\",\"😤\",\"😥\",\"😦\",\"😧\",\"😨\",\"😩\",\"😪\",\"😫\",\"😬\",\"😭\",\"😯\",\"😰\",\"😱\",\"😲\",\n",
    "               \"😳\",\"😴\",\"😷\",\"😾\",\"😿\",\"🙀\",\"💀\",\"👎\"]\n",
    "\n",
    "# Emails\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "# Mentions\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "\n",
    "#Urls\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "#Numerics\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
    "\n",
    "emoticonsDict = {}\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "            line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "            line = userMentionsRegex.sub(' USER ', line )\n",
    "            line = emailsRegex.sub(' EMAIL ', line )\n",
    "            line=urlsRegex.sub(' URL ', line)\n",
    "            line=numsRegex.sub(' NUM ',line)\n",
    "            line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
    "            line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "            words_tokens=[token for token in TweetTokenizer().tokenize(line)]                    \n",
    "            line= ' '.join(token for token in words_tokens )         \n",
    "            padded_lines.append(line)\n",
    "        padded_data=' '.join(line for line in padded_lines)\n",
    "        encoded_data=tf.compat.as_str(padded_data).split()    \n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Get raw tokens and build count dictionaries etc\"\"\"\n",
    "    count = [['<UNK>', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words))\n",
    "    dictionary = {}\n",
    "    reverse_dictionary={}\n",
    "    for word, _ in count:\n",
    "        temp = len(dictionary)\n",
    "        dictionary[word]=temp\n",
    "        reverse_dictionary[temp]=word\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "def generate_batch(data,batch_size, num_skips, skip_window ):\n",
    "    \"\"\"generate batches of data\"\"\"\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "# Model Training\n",
    "# use device='/gpu:0' to train over gpu\n",
    "\n",
    "def init_model(device='/cpu:0'):\n",
    "    \"\"\" initialize model over the input device\"\"\"\n",
    "   \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data.\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        with tf.device(device):\n",
    "                # Look up embeddings for inputs.\n",
    "                embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "                embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "                # Construct the variables for the NCE loss\n",
    "                nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "                nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.nce_loss(weights=nce_weights,\n",
    "                         biases=nce_biases,\n",
    "                         labels=train_labels,\n",
    "                         inputs=embed,\n",
    "                         num_sampled=num_sampled,\n",
    "                         num_classes=vocabulary_size))\n",
    "\n",
    "        # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "        return graph, init, train_inputs, train_labels, valid_dataset, loss, optimizer,similarity,normalized_embeddings\n",
    "\n",
    "def training(graph, init, train_inputs, train_labels, valid_dataset,loss, optimizer,similarity,normalized_embeddings):  \n",
    "    \"\"\" train model over the tweet data\"\"\"\n",
    "    num_steps = 100001\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            batch_inputs, batch_labels = generate_batch(data,batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                  # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in xrange(valid_size):\n",
    "                    valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in xrange(top_k):\n",
    "                        close_word = reverse_dictionary[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                #print(log_str)\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "    return final_embeddings\n",
    "\n",
    "\n",
    "# Export Embedding\n",
    "\n",
    "def export_embeddings(final_embeddings):\n",
    "    \"\"\" export embeddings to file\"\"\"\n",
    "    embedding_weights=pd.DataFrame(final_embeddings).round(6).reset_index()\n",
    "    word_indices_df=pd.DataFrame.from_dict(reverse_dictionary,orient='index').reset_index()\n",
    "    word_indices_df.columns=['index','word']\n",
    "    print (word_indices_df.shape,embedding_weights.shape)\n",
    "    merged=pd.merge(word_indices_df,embedding_weights).drop('index',axis=1)\n",
    "    print (merged.shape)\n",
    "    merged.to_csv(embedding_folder+'//embeddings_{}.tsv'.format(model_identifier), sep='\\t', index=False, header=False,\n",
    "                 float_format='%.6f')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Read and preprocess data\n",
      "Data size 21775861\n",
      "Vocabulary Size 303816\n",
      "Step 2: Generate batches\n",
      "Step 3 Actual Training\n",
      "Initialized\n",
      "Average loss at step  0 :  367.425262451\n",
      "Average loss at step  2000 :  201.945088058\n",
      "Average loss at step  4000 :  141.724463779\n",
      "Average loss at step  6000 :  117.003754118\n",
      "Average loss at step  8000 :  99.7307931142\n",
      "Average loss at step  10000 :  87.9533073206\n",
      "Average loss at step  12000 :  79.4605779142\n",
      "Average loss at step  14000 :  72.416219677\n",
      "Average loss at step  16000 :  64.660656188\n",
      "Average loss at step  18000 :  59.7444765675\n",
      "Average loss at step  20000 :  55.2861752367\n",
      "Average loss at step  22000 :  50.8299452715\n",
      "Average loss at step  24000 :  47.4210182767\n",
      "Average loss at step  26000 :  44.2897216585\n",
      "Average loss at step  28000 :  41.7492651081\n",
      "Average loss at step  30000 :  38.8892234578\n",
      "Average loss at step  32000 :  36.6213904314\n",
      "Average loss at step  34000 :  34.7679264398\n",
      "Average loss at step  36000 :  32.6908995425\n",
      "Average loss at step  38000 :  31.3322881873\n",
      "Average loss at step  40000 :  29.7221862748\n",
      "Average loss at step  42000 :  27.5784047663\n",
      "Average loss at step  44000 :  26.72313991\n",
      "Average loss at step  46000 :  25.5061762455\n",
      "Average loss at step  48000 :  24.402114465\n",
      "Average loss at step  50000 :  23.3068728504\n",
      "Average loss at step  52000 :  22.3975908833\n",
      "Average loss at step  54000 :  21.2120319414\n",
      "Average loss at step  56000 :  20.0414454837\n",
      "Average loss at step  58000 :  19.928457201\n",
      "Average loss at step  60000 :  19.1149211659\n",
      "Average loss at step  62000 :  18.4641330223\n",
      "Average loss at step  64000 :  18.0733943765\n",
      "Average loss at step  66000 :  17.8092929416\n",
      "Average loss at step  68000 :  16.603091965\n",
      "Average loss at step  70000 :  16.1223484001\n",
      "Average loss at step  72000 :  16.1630183713\n",
      "Average loss at step  74000 :  15.4346868527\n",
      "Average loss at step  76000 :  14.4472491996\n",
      "Average loss at step  78000 :  14.3360542567\n",
      "Average loss at step  80000 :  13.9894238446\n",
      "Average loss at step  82000 :  13.9354281099\n",
      "Average loss at step  84000 :  13.1424811659\n",
      "Average loss at step  86000 :  12.9347420553\n",
      "Average loss at step  88000 :  12.8305291088\n",
      "Average loss at step  90000 :  12.2188944339\n",
      "Average loss at step  92000 :  12.027398577\n",
      "Average loss at step  94000 :  11.6895120076\n",
      "Average loss at step  96000 :  11.5113231206\n",
      "Average loss at step  98000 :  11.3598829354\n",
      "Average loss at step  100000 :  11.2954569099\n",
      "Total time taken to generate embedding 148.267258\n",
      "Step 4: Export Embeddings\n",
      "(303816, 2) (303815, 51)\n",
      "(303815, 51)\n"
     ]
    }
   ],
   "source": [
    "print ('Step 1: Read and preprocess data')\n",
    "vocabulary = read_data(training_filename)\n",
    "vocabulary_size=len(set(vocabulary)) # Not excluding anything from the library # add 1 voc size\n",
    "print ('Data size', len(vocabulary))\n",
    "print ('Vocabulary Size', vocabulary_size +1) # including the count of Unknown i.e. UNK token\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "data_index = 0\n",
    "\n",
    "print ('Step 2: Generate batches')\n",
    "batch, labels = generate_batch(data,batch_size=8, num_skips=2, skip_window=1)\n",
    "\n",
    "print ('Step 3 Actual Training')\n",
    "start=timer()\n",
    "graph, init, train_inputs, train_labels, valid_dataset,loss, optimizer,similarity,normalized_embeddings=init_model()\n",
    "final_embedding=training(graph, init, train_inputs, train_labels, valid_dataset,loss, optimizer,similarity,normalized_embeddings)\n",
    "end=timer()\n",
    "print ('Total time taken to generate embedding' , (end-start))\n",
    "\n",
    "print ('Step 4: Export Embeddings')\n",
    "embedding_df=export_embeddings(final_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

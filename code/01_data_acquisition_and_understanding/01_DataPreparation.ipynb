{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "from tempfile import mktemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=r'C:\\Users\\ds1\\Documents\\AzureML'\n",
    "base_folder='data'\n",
    "\n",
    "# URL to download the sentiment140 dataset\n",
    "data_url='http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to download and process data\n",
    "\n",
    "def change_base_dir(base_dir_path):\n",
    "    \"\"\" Change the working directopry of the code\"\"\"\n",
    "    \n",
    "    if not os.path.exists(base_dir_path):\n",
    "        print ('creating directory', base_dir_path)\n",
    "        os.makedirs(base_dir_path)\n",
    "    print ('Changing base directory to ', base_dir_path)\n",
    "    os.chdir(base_dir_path)\n",
    "\n",
    "def download_data(download_url, filename='downloaded_data.zip'):\n",
    "    \"\"\" Download and extract data \"\"\"\n",
    "    \n",
    "    downloaded_filename = os.path.join('.', filename)\n",
    "    print ('Step 1: Downloading data')\n",
    "    urllib.request.urlretrieve(download_url,downloaded_filename)\n",
    "    print ('Step 2: Extracting data')\n",
    "    zipfile=ZipFile(downloaded_filename)\n",
    "    zipfile.extractall('./')\n",
    "    zipfile.close()\n",
    "\n",
    "def extract_tweets_and_labels(filename ):\n",
    "    \"\"\" Extract tweets and labels from the downloaded data\"\"\"\n",
    "    \n",
    "    print ('Step 3: Reading the data as a dataframe')\n",
    "    df=pd.read_csv(filename, header=None, encoding='iso-8859-1')    \n",
    "    df.columns=['Label','TweetId','Date','Query','User','Text']\n",
    "    print ('Read {} lines'.format(df.shape[0]))\n",
    "    print ('Discarding neutral tweets')\n",
    "    df=df[df.Label!=2]\n",
    "    print ('No of lines in the data after filtering neutral tweets: {}'.format(df.shape[0]))\n",
    "    print ('Step 4: Shuffling the data')\n",
    "    train_length=int(df.shape[0]*0.8)    \n",
    "    df=df.sample(frac=1) # reshuffling the data\n",
    "      \n",
    "    df['Text']=df['Text'].astype(str).apply(lambda x:x.strip())#.encode('ascii','ignore')#str.decode('utf8','ignore')#.str.encode('ascii','ignore')\n",
    "    print (df.head())\n",
    "    print ('Step 5: Dividing into test and train datasets')\n",
    "    df_train = df.iloc[:train_length, :]\n",
    "    df_test = df.iloc[train_length:, :]    \n",
    "    \n",
    "    print ('Step 6: Exporting the train and test datasets')    \n",
    "    print ('Exporting training data of rows {}'.format(df_train.shape[0]))\n",
    "    export_prefix='training'\n",
    "    df_train[['Label']].to_csv(export_prefix+'_label.csv', header=False, index=False)\n",
    "    df_train[['Text']].to_csv(export_prefix+'_text.csv', header=False, index=False)\n",
    "    print ('Target distribution in the training data is as follows')\n",
    "    print ('\\n',df_train['Label'].value_counts()) \n",
    "    \n",
    "    print ('Exporting training data of rows {}'.format(df_test.shape[0]))\n",
    "    export_prefix='testing'\n",
    "    df_test[['Label']].to_csv(export_prefix+'_label.csv', header=False, index=False)\n",
    "    df_test[['Text']].to_csv(export_prefix+'_text.csv', header=False, index=False)\n",
    "    print ('Target distribution in the testing data is as follows')\n",
    "    print ('\\n',df_test['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing base directory to  C:\\Users\\ds1\\Documents\\AzureML\\data\n",
      "Step 1: Downloading data\n",
      "Step 2: Extracting data\n",
      "Step 3: Reading the data as a dataframe\n",
      "Read 1600000 lines\n",
      "Discarding neutral tweets\n",
      "No of lines in the data after filtering neutral tweets: 1600000\n",
      "Step 4: Shuffling the data\n",
      "         Label     TweetId                          Date     Query  \\\n",
      "583617       0  2215002587  Wed Jun 17 17:32:06 PDT 2009  NO_QUERY   \n",
      "1530030      4  2177699730  Mon Jun 15 06:33:19 PDT 2009  NO_QUERY   \n",
      "127209       0  1834675142  Mon May 18 03:49:23 PDT 2009  NO_QUERY   \n",
      "19557        0  1556879231  Sun Apr 19 00:48:15 PDT 2009  NO_QUERY   \n",
      "353007       0  2031663185  Thu Jun 04 10:08:26 PDT 2009  NO_QUERY   \n",
      "\n",
      "                User                                               Text  \n",
      "583617    dwaitsbaby                                         work sucks  \n",
      "1530030    minxkitty           @K8loulee hello hun! Nice to 'meet' you!  \n",
      "127209    TferThomas  @melissaox Ah I understand now. I will admit t...  \n",
      "19557         Zetura  This is my last hour on Twitter for the next week  \n",
      "353007   xoxolaurenn  Garbage trucks are really loud :/ I just got a...  \n",
      "Step 5: Dividing into test and train datasets\n",
      "Step 6: Exporting the train and test datasets\n",
      "Exporting training data of rows 1280000\n",
      "Target distribution in the training data is as follows\n",
      "\n",
      " 4    640058\n",
      "0    639942\n",
      "Name: Label, dtype: int64\n",
      "Exporting training data of rows 320000\n",
      "Target distribution in the testing data is as follows\n",
      "\n",
      " 0    160058\n",
      "4    159942\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Download and processing the data\n",
    "\n",
    "base_dir_path=base_path+'\\\\'+base_folder\n",
    "change_base_dir(base_dir_path)\n",
    "download_data(data_url)\n",
    "extract_tweets_and_labels('training.1600000.processed.noemoticon.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
